{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osIWaXxdYxI-"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install scispacy negspacy memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0flMTzBNEu6"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from negspacy.negation import Negex\n",
    "from scispacy.linking import EntityLinker\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "from memory_profiler import profile\n",
    "import time\n",
    "import sys\n",
    "import psutil \n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJslaVKeaeyq"
   },
   "outputs": [],
   "source": [
    "extractor_to_use = 5\n",
    "# Functions needed for entity extraction models\n",
    "f=open('acronyms.txt','r')\n",
    "acronyms_dict={}\n",
    "l=f.readline()\n",
    "l=f.readline()\n",
    "while l:\n",
    "    l1=l.split(\"\\t\")\n",
    "    acronyms_dict[l1[0].lower()]=l1[1].lower()\n",
    "    l=f.readline()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZQwSALubNzu"
   },
   "outputs": [],
   "source": [
    "#lemmatizing the notes to capture all forms of negation(e.g., deny: denies, denying)\n",
    "def lemmatize(note, nlp):\n",
    "    doc = nlp(note)\n",
    "    lemNote = [wd.lemma_ for wd in doc]\n",
    "    return \" \".join(lemNote)\n",
    "\n",
    "def acronym_to_full_text(row,acronyms_dict):\n",
    "    final_text = \"\"\n",
    "  \n",
    "    for word in row.split(\" \"):\n",
    "        if word.strip() in acronyms_dict.keys():\n",
    "            final_text=final_text+acronyms_dict[word.strip()]+\" \"\n",
    "        else:\n",
    "            final_text=final_text+word.strip()+\" \"\n",
    "    return final_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNzjXsVAbc6d"
   },
   "outputs": [],
   "source": [
    "def preprocessing_matching_algorithm():\n",
    "   # Input file is this diagnosis one where we are taking one, bi, and ngram values.\n",
    "   # combination helps in making newer words for each word/s and combination/s.\n",
    "    Full_data=pd.ExcelFile(r'Diagnosis N Gram MAX 3 12April22upd2.xlsx',engine='openpyxl')\n",
    "\n",
    "    one_gram=pd.read_excel(Full_data,sheet_name='Unigram')\n",
    "\n",
    "    one_gram=one_gram.dropna()\n",
    "    one_gram.rename(columns={'Category(Investigation/Complaint/Diagnosis)':'Category(Investigation/Complaint/Diagnosis/Procedure)'},inplace=True)\n",
    "    one_gram.dropna(subset=['Label'],inplace=True)\n",
    "    one_gram=one_gram[['word','Category(Investigation/Complaint/Diagnosis/Procedure)','Label']]\n",
    "\n",
    "\n",
    "    Bigram=pd.read_excel(Full_data,sheet_name='Bigram')\n",
    "    Bigram['Label']=np.where(Bigram['Label2'].isna(),Bigram['Label1'],Bigram['Label1']+\" \"+Bigram['Label2'])\n",
    "    Bigram.rename(columns={'Category(Investigation/Complaint/Diagnosis)':'Category(Investigation/Complaint/Diagnosis/Procedure)'},inplace=True)\n",
    "    Bigram.dropna(subset=['Label'],inplace=True)\n",
    "    Bigram=Bigram[['word','Category(Investigation/Complaint/Diagnosis/Procedure)','Label']]\n",
    "\n",
    "    \n",
    "    Trigram=pd.read_excel(Full_data,sheet_name='Trigram')\n",
    "    Trigram.fillna(\"\",inplace=True)\n",
    "    Trigram['Label']=Trigram['Label1']+\" \"+Trigram['Label2']+\" \"+Trigram['Label3']\n",
    "    Trigram['Label']=Trigram['Label'].str.strip()\n",
    "    Trigram['len']=Trigram.Label.str.len()\n",
    "    Trigram=Trigram[Trigram['len']>0]\n",
    "    Trigram.dropna(subset=['Label'],inplace=True)\n",
    "    Trigram=Trigram[['word','Category(Investigation/Complaint/Diagnosis/Procedure)','Label']]\n",
    "    \n",
    "    n_gram_sheet=one_gram.append(Bigram)\n",
    "    n_gram_sheet=n_gram_sheet.append(Trigram)\n",
    "    n_gram_sheet.fillna(\"\",inplace=True)\n",
    "    n_gram_sheet=n_gram_sheet.replace('nan',\"\")\n",
    "    n_gram_sheet['len']=n_gram_sheet['Category(Investigation/Complaint/Diagnosis/Procedure)'].str.len()\n",
    "    n_gram_sheet=n_gram_sheet[n_gram_sheet['len']>0]\n",
    "    n_gram_sheet['label']=n_gram_sheet['Category(Investigation/Complaint/Diagnosis/Procedure)']+\":\"+n_gram_sheet['Label']\n",
    "    n_gram_sheet.rename(columns={'word':'pattern'},inplace=True)\n",
    "    \n",
    "    investigation=pd.read_excel('UnifiedBiomarkerMapping_1.xlsx',engine='openpyxl',sheet_name='Top TestNames')\n",
    "    investigation.dropna()\n",
    "    investigation.rename(columns={'lab test names':'pattern'},inplace=True)\n",
    "    investigation['pattern']=investigation['pattern'].astype(str)\n",
    "\n",
    "    investigation['label']='Investigation:'+(investigation['pattern'])\n",
    "    n_gram_sheet=n_gram_sheet.append(investigation)\n",
    "\n",
    "    drugs=pd.read_csv('final_drug_list.csv')\n",
    "    drugs.dropna()\n",
    "    drugs.rename(columns={'value_upd_3':'pattern'},inplace=True)\n",
    "    drugs['label']='Drugs:'+drugs['pattern']\n",
    "    print(drugs)\n",
    "    n_gram_sheet=n_gram_sheet.append(drugs)\n",
    "    n_gram_sheet=n_gram_sheet[['label','pattern']]\n",
    "\n",
    "    gram_dict=n_gram_sheet.to_dict(\"record\")\n",
    "  \n",
    "    length_gram_dict = len(gram_dict)\n",
    "    for i in range(0,length_gram_dict):\n",
    "        pattern = gram_dict[i]['pattern']\n",
    "        pattern = acronym_to_full_text(pattern,acronyms_dict)\n",
    "        pattern_list = pattern.split()\n",
    "        l=[]\n",
    "        l1 = []\n",
    "        for j in range(0,len(pattern_list)):\n",
    "            d={}\n",
    "            d[\"LOWER\"]=pattern_list[j]\n",
    "            l.append(d)\n",
    "            l1.append(d)\n",
    "            if j!=len(pattern_list)-1:\n",
    "                l1.append({\"IS_PUNCT\": True})\n",
    "        gram_dict.append({}) \n",
    "        gram_dict[i]['pattern'] = l\n",
    "        \n",
    "        gram_dict[i+length_gram_dict]['pattern'] = l1\n",
    "        gram_dict[i+length_gram_dict]['label'] = gram_dict[i]['label']\n",
    "    #print(gram_dict)\n",
    "    return gram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCMHJGljbfmp"
   },
   "outputs": [],
   "source": [
    "#Reference https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt\n",
    "valid_semantic_types = ['T109','T116','T020','T190','T017','T195','T053','T038','T030','T019','T200','T060','T047','T129','T037','T059','T034','T048','T046','T121','T061','T184','T033','T191' ]\n",
    "complaint_semantic_types = ['T184','T033']\n",
    "diagnosis_semantic_types = ['T020','T190','T053','T038','T029','T023','T030','T019','T047','T129','T037','T048','T046','T191']\n",
    "investigation_semantic_types=['T017','T034','T060','T059','T061']\n",
    "procedure_semantic_types=[]\n",
    "drugs_semantic_types = ['T195','T200','T116','T109','T122']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBh6WhG9bizH",
    "outputId": "9515323a-9833-42f0-b23a-86d383e57b4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0        pattern                label\n",
      "0               0  betacap tr 40  Drugs:betacap tr 40\n",
      "1               1      ferium xt      Drugs:ferium xt\n",
      "2               2    pexep cr 12    Drugs:pexep cr 12\n",
      "3               3         forcan         Drugs:forcan\n",
      "4               4     disperzyme     Drugs:disperzyme\n",
      "...           ...            ...                  ...\n",
      "95038       85570       Puron 12       Drugs:Puron 12\n",
      "95039       85590     Sinarest 0     Drugs:Sinarest 0\n",
      "95040       85756        Entof 0        Drugs:Entof 0\n",
      "95041       85859     Diflucor 2     Drugs:Diflucor 2\n",
      "95042       85995       Biomus 0       Drugs:Biomus 0\n",
      "\n",
      "[95043 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "patterns = preprocessing_matching_algorithm()\n",
    "\n",
    "if extractor_to_use==1 or extractor_to_use == 3:\n",
    "    nlp1 = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "    nlp1.add_pipe(\"scispacy_linker\", config={\"linker_name\": \"umls\",\"max_entities_per_mention\": 1,\"threshold\": 0.9})\n",
    "    nlp1.add_pipe(\"negex\",config={\"chunk_prefix\": [\"no\"]})\n",
    "if extractor_to_use == 2 or extractor_to_use == 3:\n",
    "    nlp2 = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "    nlp2.add_pipe(\"negex\",config={\"chunk_prefix\": [\"no\"]})\n",
    "if extractor_to_use == 4:\n",
    "    nlp3 = English()\n",
    "    ruler = nlp3.add_pipe(\"entity_ruler\", config={\"overwrite_ents\":True})\n",
    "    ruler.add_patterns(patterns)\n",
    "    nlp3.add_pipe(\"negex\",config={\"chunk_prefix\": [\"no\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfsh6RN9cTH6"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUT842omZ9Cz"
   },
   "outputs": [],
   "source": [
    "# RG Files -> SVAAS -> HC Folder -> Complain List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbhxMxuWbnQ5",
    "outputId": "cf2c1e25-df0e-4476-afac-04001fa7eb7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/tmpwggnt9pr\n",
      "Finished download, copying /tmp/tmpwggnt9pr to cache at /root/.scispacy/datasets/e9f7327283e43f0482f7c0c71b71dec278a58ccb3ffdd03c2c2350159e7ef146.f2a350ad19015b2591545f7feeed6a6d6d2fffcd635d868a5d7fc0dfc3cadfd8.tfidf_vectors_sparse.npz\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/nmslib_index.bin not found in cache, downloading to /tmp/tmp6l9vi79_\n",
      "Finished download, copying /tmp/tmp6l9vi79_ to cache at /root/.scispacy/datasets/f48455d6c79262057cce66b4619123c2b558b21092d42fac97f47bb99a5b8f9f.dd70d3dffe7d90d7ac8914460e16a48375dab32485fb6313a34e6fbcaf53218b.nmslib_index.bin\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectorizer.joblib not found in cache, downloading to /tmp/tmpmm01jji1\n",
      "Finished download, copying /tmp/tmpmm01jji1 to cache at /root/.scispacy/datasets/8c32f1e7ddf19ec695c321f68a71f06a191aec8efcf6b645b78fa6250d8d81d3.89019b4a62a096f33ea23677557a4cde66ebc8228f30afabac38e32f834020dc.tfidf_vectorizer.joblib\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/concept_aliases.json not found in cache, downloading to /tmp/tmpnu9y6c_c\n",
      "Finished download, copying /tmp/tmpnu9y6c_c to cache at /root/.scispacy/datasets/1428ec15d3b1061731ea273c03699130b3d6b90948993e74bda66af605ff8e2a.aeb7a686c654df6bccb6c2c23d3eda3eb381daaefda4592b58158d0bee53b352.concept_aliases.json\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/kbs/2020-10-09/umls_2020_aa_cat0129.jsonl not found in cache, downloading to /tmp/tmp0i0j90pv\n",
      "Finished download, copying /tmp/tmp0i0j90pv to cache at /root/.scispacy/datasets/4d7fb8fcae1035d1e0a47d9072b43d5a628057d35497fbfb2499b4b7b2dd4dd7.05ec7eef12f336d4666da85b7fa69b9401883a7dd4244473f7b88b413ccbba03.umls_2020_aa_cat0129.jsonl\n",
      "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/umls_semantic_type_tree.tsv not found in cache, downloading to /tmp/tmpjqrsl6gi\n",
      "Finished download, copying /tmp/tmpjqrsl6gi to cache at /root/.scispacy/datasets/21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv\n",
      "Ruler 1:  <spacy.pipeline.entityruler.EntityRuler object at 0x7fbc3ae2c730>\n"
     ]
    }
   ],
   "source": [
    "if extractor_to_use == 5 or extractor_to_use == 7:\n",
    "    nlp4 = spacy.load(\"en_core_sci_lg\")\n",
    "    nlp4.add_pipe(\"scispacy_linker\", config={\"linker_name\": \"umls\",\"max_entities_per_mention\": 1,\"threshold\": 0.9})\n",
    "    ruler1 = nlp4.add_pipe(\"entity_ruler\", config={\"overwrite_ents\":True})\n",
    "    ruler1.add_patterns(patterns)\n",
    "    print(\"Ruler 1: \", ruler1)\n",
    "    nlp4.add_pipe(\"negex\",config={\"chunk_prefix\": [\"no\"]})\n",
    "\n",
    "if extractor_to_use == 6 or extractor_to_use == 7:\n",
    "    nlp5 = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "    ruler2 = nlp5.add_pipe(\"entity_ruler\", config={\"overwrite_ents\":True})\n",
    "    ruler2.add_patterns(patterns)\n",
    "    nlp5.add_pipe(\"negex\",config={\"chunk_prefix\": [\"no\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJzJYopgfWxJ"
   },
   "outputs": [],
   "source": [
    "def entity_extraction(row,value):\n",
    " #   print(row[value])\n",
    "    val_list =row[value].split(\"##\")\n",
    "   # print(val_list)\n",
    "    complaint = []\n",
    "    diagnosis = []\n",
    "    investigation=[]\n",
    "    procedure = []\n",
    "    drugs=[]\n",
    "        \n",
    "    for val in val_list:\n",
    "        regexpattern = r'\\.\\.+'\n",
    "        val = re.sub(regexpattern, ' ', val)\n",
    "        val  = ' '.join(acronym_to_full_text(val.strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split())\n",
    "\n",
    "        #print(\"val is \",val)\n",
    "            \n",
    "        if extractor_to_use==1 or extractor_to_use == 3:\n",
    "            doc1 = nlp1(val)\n",
    "        if extractor_to_use==2 or extractor_to_use ==3 :\n",
    "            doc2 = nlp2(val)\n",
    "        if extractor_to_use==4:\n",
    "            doc3 = nlp3(val)\n",
    "        if extractor_to_use==5 or extractor_to_use==7:\n",
    "            doc4 = nlp4(val)\n",
    "        if extractor_to_use==6 or extractor_to_use==7:\n",
    "            doc5=nlp5(val)\n",
    "        \n",
    "       \n",
    "        if extractor_to_use == 4 or extractor_to_use==5 or extractor_to_use==6 or extractor_to_use==7:\n",
    "            \n",
    "            label=[]\n",
    "            text=[]\n",
    "            ent_list=[]\n",
    "            if extractor_to_use ==4:\n",
    "                ent_list.append(doc3.ents)\n",
    "            if extractor_to_use == 5 or extractor_to_use==7:\n",
    "                ent_list.append(doc4.ents)\n",
    "            if extractor_to_use==6 or extractor_to_use == 7:\n",
    "                ent_list.append(doc5.ents)\n",
    "            \n",
    "            for ent_tuple in ent_list:\n",
    "                for ent in ent_tuple:\n",
    "                   # print(ent,ent.label_,ent.text,ent._.negex)\n",
    "                    if ent._.negex == 0 and \":\" in ent.label_:\n",
    "                        label.append(str(ent.label_))\n",
    "                        text.append(ent.text)\n",
    "            \n",
    "            if len(label) > 0:\n",
    "                all_labels=pd.DataFrame(label,columns =['Names'])\n",
    "                all_labels[['Category', 'insight']] = all_labels['Names'].str.split(':', 1, expand=True)\n",
    "                all_labels['Category']=all_labels.Category.str.strip()\n",
    "                all_labels['insight']=all_labels.insight.str.strip()\n",
    "                final_label=all_labels[['Category','insight']].groupby(['Category'])['insight'].apply(','.join).reset_index()\n",
    "                #print(final_label)\n",
    "                for i in range(len(final_label)):    \n",
    "                    if final_label.iloc[i]['Category']=='Complaint':\n",
    "                        if final_label.iloc[i]['insight'] not in complaint:\n",
    "                            complaint.append(final_label.iloc[i]['insight'])\n",
    "                    elif final_label.iloc[i]['Category']=='Investigation':\n",
    "                        if final_label.iloc[i]['insight'] not in investigation:\n",
    "                            investigation.append(final_label.iloc[i]['insight'])\n",
    "                    elif final_label.iloc[i]['Category']=='Diagnosis':\n",
    "                        if final_label.iloc[i]['insight'] not in diagnosis:\n",
    "                            diagnosis.append(final_label.iloc[i]['insight'])\n",
    "                    elif final_label.iloc[i]['Category']=='Drugs':\n",
    "                        if final_label.iloc[i]['insight'] not in drugs:\n",
    "                            drugs.append(final_label.iloc[i]['insight'])\n",
    "                    elif final_label.iloc[i]['Category']=='Procedure':\n",
    "                        if final_label.iloc[i]['insight'] not in procedure:\n",
    "                            procedure.append(final_label.iloc[i]['insight']) \n",
    "          \n",
    "        if extractor_to_use ==1 or extractor_to_use == 3 or extractor_to_use == 5 or extractor_to_use==7:\n",
    "            if extractor_to_use ==1 or extractor_to_use == 3:\n",
    "                ent_list = doc1.ents\n",
    "                linker = nlp1.get_pipe(\"scispacy_linker\")\n",
    "            else:\n",
    "                ent_list = doc4.ents\n",
    "                linker = nlp4.get_pipe(\"scispacy_linker\")\n",
    "            #print(ent_list)\n",
    "            for entity in ent_list:\n",
    "                #print(entity.text,entity.label_,entity._.negex,entity._.kb_ents)\n",
    "                if entity._.negex == 0 and entity.label_=='ENTITY':\n",
    "                #Link to UMLS knowledge baseT048\n",
    "                \n",
    "             #   print(entity._.kb_ents)\n",
    "                    for kb_entry in entity._.kb_ents:\n",
    "                        sem_type=linker.kb.cui_to_entity[kb_entry[0]].types[0]\n",
    "                        #print(entity,sem_type,linker.kb.cui_to_entity[kb_entry[0]].types)\n",
    "                        #print(linker.kb.cui_to_entity[kb_entry[0]].canonical_name)\n",
    "                        if sem_type in valid_semantic_types:\n",
    "                            if sem_type in complaint_semantic_types:\n",
    "                                if str(entity.text) not in complaint:\n",
    "                                    complaint.append(str(entity.text))\n",
    "                            elif sem_type in diagnosis_semantic_types:\n",
    "                                if str(entity.text) not in diagnosis:\n",
    "                                    diagnosis.append(str(entity.text))\n",
    "                            elif sem_type in investigation_semantic_types:\n",
    "                                if str(entity.text) not in investigation:\n",
    "                                    investigation.append(str(entity.text))\n",
    "                            elif sem_type in procedure_semantic_types:\n",
    "                                if str(entity.text) not in procedure:\n",
    "                                    procedure.append(str(entity.text))\n",
    "                            elif sem_type in drugs_semantic_types:\n",
    "                                if str(entity.text) not in drugs:\n",
    "                                    drugs.append(str(entity.text))\n",
    "            \n",
    "                \n",
    "        if extractor_to_use==2 or extractor_to_use ==3 or extractor_to_use==6 or extractor_to_use==7:\n",
    "            if extractor_to_use == 2 or extractor_to_use == 3:\n",
    "                ent_list = doc2.ents\n",
    "            else:\n",
    "                ent_list = doc5.ents\n",
    "            for entity in ent_list:\n",
    "                if entity._.negex == 0:\n",
    "                    if entity.label_ == 'DISEASE':\n",
    "                        if str(entity.text) not in diagnosis and str(entity.text) not in complaint:\n",
    "                            diagnosis.append(str(entity.text))\n",
    "                    elif entity.label_ == 'CHEMICAL':\n",
    "                        if str(entity.text) not in drugs:\n",
    "                            drugs.append(str(entity.text))\n",
    "        \n",
    "       \n",
    "                    \n",
    "     \n",
    "       \n",
    "    row['COMPLAINT'] = \",\".join(complaint)\n",
    "    row['DIAGNOSIS'] = \",\".join(diagnosis)\n",
    "    row['INVESTIGATION'] = \",\".join(investigation)\n",
    "    row['PROCEDURE'] = \",\".join(procedure)\n",
    "    row['DRUGS'] = ','.join(drugs)\n",
    "    \n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssCMk9vGfbAe"
   },
   "outputs": [],
   "source": [
    "def clean_data(row):\n",
    "    rowlist = row.split(\",\")\n",
    "    modrowlist=[]\n",
    "    for r in rowlist:\n",
    "        if r.strip() not in modrowlist and r.strip()!=\"\":\n",
    "            modrowlist.append(r.strip())\n",
    "    return \",\".join(modrowlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BTMTsIGffTG"
   },
   "outputs": [],
   "source": [
    "def generateResults():\n",
    "    data2 = pd.read_csv(\"dt.csv\")\n",
    "    data2.fillna(\"NA\", inplace=True)\n",
    "\n",
    "    combined_values = data2['complaints'].values + \",\" + data2['advice'].values + \",\" + data2['diagnosis'].values\n",
    "\n",
    "    data_req = pd.DataFrame(np.c_[data2['brdg_appt_conslt_sid'].values, combined_values], \n",
    "                              columns=['brdg_appt_consult_id', 'combined_diagnosis']\n",
    "                            )\n",
    "    \n",
    "    data_req['combined_diagnosis'] = data_req['combined_diagnosis'].apply(lambda x: x.lower())\n",
    "    data = data_req.copy()\n",
    "    del data_req\n",
    "    \n",
    "    data['combined_diagnosis']=data['combined_diagnosis'].astype(str)\n",
    "    data.replace(\"\", \"na\", inplace=True)\n",
    "    print(data.shape)\n",
    "    data.dropna(how='all',axis=0,inplace=True)\n",
    "    data.dropna(how='all', axis=1, inplace=True)\n",
    "    print(data.shape)\n",
    "    df_ext_Remarks=data.apply(lambda row:entity_extraction(row,'combined_diagnosis'),axis=1)\n",
    "    df_ext_Remarks.to_csv(\"remarks.csv\", index=False)\n",
    "    return df_ext_Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMappedComplainDataFrame(complain_data, original_data):\n",
    "    df_result = generateResults()\n",
    "    df_result.fillna(\"NaN\", inplace=True)\n",
    "    df_result['DIAGNOSIS'] = df_result['DIAGNOSIS'].apply(lambda x: x.lower())\n",
    "    \n",
    "    df_result[\"DIAGNOSIS\"] = df_result[\"DIAGNOSIS\"].str.split(\",\")\n",
    "    df_result = df_result.explode(\"DIAGNOSIS\")\n",
    "    \n",
    "    assert \".csv\" in complain_data, \"Make sure to insert a csv file.\"\n",
    "    assert \".csv\" in original_data, \"Make sure to insert a csv file.\"\n",
    "    data_req = pd.read_csv(complain_data)\n",
    "    \n",
    "    brdg = pd.read_csv(original_data)\n",
    "    df_result.rename(columns={'brdg_appt_consult_id':'brdg_appt_conslt_sid'}, inplace=True)\n",
    "    \n",
    "    cols = [\"brdg_appt_conslt_sid\", \"DIAGNOSIS\", \"COMPLAINT\"]\n",
    "\n",
    "    df_result = df_result[cols]\n",
    "    df_result.columns = ['brdg_appt_conslt_sid', 'mapped_DIAGNOSIS', 'mapped_COMPLAINT']\n",
    "    \n",
    "    brdg.merge(df_result, on=\"brdg_appt_conslt_sid\", how=\"left\").to_csv(\"merged_csv_complain_v2.csv\", index=False)\n",
    "\n",
    "    df_final = pd.read_csv(\"merged_csv_complain_v2.csv\")\n",
    "    print(\"Final dataframe has been saved with the name of output.csv\")\n",
    "    df_final.replace(\"nan\", \"\").fillna(\" \").to_csv(\"output.csv\", index=False)\n",
    "    \n",
    "    df_req = pd.read_csv(complain_data)\n",
    "    \n",
    "    df_final= df_final.merge(df_req, left_on=[\"mapped_DIAGNOSIS\"], right_on=[\"Words\"], how=\"left\")\n",
    "    \n",
    "    return df_final.replace(\"nan\", \"\").fillna(\" \")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(\"dt.csv\")\n",
    "complain_data = pd.read_csv(\"full_complain_list.csv\")\n",
    "\n",
    "\n",
    "df_final = generateMappedComplainDataFrame(complain_data, original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brdg_appt_conslt_sid</th>\n",
       "      <th>appointment_id</th>\n",
       "      <th>consultation_id</th>\n",
       "      <th>appointment_dt_sid</th>\n",
       "      <th>appointment_datetime</th>\n",
       "      <th>appointment_status</th>\n",
       "      <th>appointment_type</th>\n",
       "      <th>svaas_appointment_id</th>\n",
       "      <th>vendor_sid</th>\n",
       "      <th>doctor_id</th>\n",
       "      <th>...</th>\n",
       "      <th>bp_sys</th>\n",
       "      <th>pulse</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>bmi</th>\n",
       "      <th>row_insert_dt</th>\n",
       "      <th>row_update_dt</th>\n",
       "      <th>consultation_doc_ref</th>\n",
       "      <th>mapped_DIAGNOSIS</th>\n",
       "      <th>mapped_COMPLAINT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2253</td>\n",
       "      <td>-1</td>\n",
       "      <td>20220208</td>\n",
       "      <td>2/8/2022 19:45</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>InPerson</td>\n",
       "      <td>D-HX0802-1146-4247758236</td>\n",
       "      <td>1239</td>\n",
       "      <td>584</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/7/2022 14:30</td>\n",
       "      <td>9/7/2022 14:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4551</td>\n",
       "      <td>-1</td>\n",
       "      <td>20220816</td>\n",
       "      <td>8/16/2022 10:10</td>\n",
       "      <td>declined</td>\n",
       "      <td>OnlineConsultation</td>\n",
       "      <td>D-HX1608-0942-2283404928</td>\n",
       "      <td>1790</td>\n",
       "      <td>1233</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/7/2022 14:30</td>\n",
       "      <td>9/7/2022 14:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3731</td>\n",
       "      <td>899</td>\n",
       "      <td>20220525</td>\n",
       "      <td>5/25/2022 19:00</td>\n",
       "      <td>completed</td>\n",
       "      <td>OnlineConsultation</td>\n",
       "      <td>D-HX2505-1302-3731620448</td>\n",
       "      <td>726</td>\n",
       "      <td>1247</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/7/2022 14:30</td>\n",
       "      <td>9/7/2022 14:30</td>\n",
       "      <td>1653486348_pdf_Psn4t6MIZJAFW3uYoR7e.pdf</td>\n",
       "      <td>htn</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2241</td>\n",
       "      <td>-1</td>\n",
       "      <td>20220207</td>\n",
       "      <td>2/7/2022 18:00</td>\n",
       "      <td>completed</td>\n",
       "      <td>InPerson</td>\n",
       "      <td>D-HX0702-1021-6989648006</td>\n",
       "      <td>849</td>\n",
       "      <td>1107</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/7/2022 14:30</td>\n",
       "      <td>9/7/2022 14:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2586</td>\n",
       "      <td>591</td>\n",
       "      <td>20220307</td>\n",
       "      <td>3/7/2022 17:30</td>\n",
       "      <td>completed</td>\n",
       "      <td>InPerson</td>\n",
       "      <td>D-HX0703-0956-1922150952</td>\n",
       "      <td>1838</td>\n",
       "      <td>645</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/7/2022 14:30</td>\n",
       "      <td>9/7/2022 14:30</td>\n",
       "      <td>1646656625_pdf_t9oIOr8FNXGkmLnE245S.pdf</td>\n",
       "      <td>pityriasis</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   brdg_appt_conslt_sid  appointment_id  consultation_id  appointment_dt_sid  \\\n",
       "0                     1            2253               -1            20220208   \n",
       "1                     2            4551               -1            20220816   \n",
       "2                     3            3731              899            20220525   \n",
       "3                     4            2241               -1            20220207   \n",
       "4                     5            2586              591            20220307   \n",
       "\n",
       "  appointment_datetime appointment_status    appointment_type  \\\n",
       "0       2/8/2022 19:45          cancelled            InPerson   \n",
       "1      8/16/2022 10:10           declined  OnlineConsultation   \n",
       "2      5/25/2022 19:00          completed  OnlineConsultation   \n",
       "3       2/7/2022 18:00          completed            InPerson   \n",
       "4       3/7/2022 17:30          completed            InPerson   \n",
       "\n",
       "       svaas_appointment_id  vendor_sid  doctor_id  ...  bp_sys  pulse  \\\n",
       "0  D-HX0802-1146-4247758236        1239        584  ...     NaN    NaN   \n",
       "1  D-HX1608-0942-2283404928        1790       1233  ...     NaN    NaN   \n",
       "2  D-HX2505-1302-3731620448         726       1247  ...     NaN    NaN   \n",
       "3  D-HX0702-1021-6989648006         849       1107  ...     NaN    NaN   \n",
       "4  D-HX0703-0956-1922150952        1838        645  ...     NaN    NaN   \n",
       "\n",
       "   weight  height  bmi   row_insert_dt   row_update_dt  \\\n",
       "0     NaN     NaN  NaN  9/7/2022 14:30  9/7/2022 14:30   \n",
       "1     NaN     NaN  NaN  9/7/2022 14:30  9/7/2022 14:30   \n",
       "2     NaN     NaN  NaN  9/7/2022 14:30  9/7/2022 14:30   \n",
       "3     NaN     NaN  NaN  9/7/2022 14:30  9/7/2022 14:30   \n",
       "4     NaN     NaN  NaN  9/7/2022 14:30  9/7/2022 14:30   \n",
       "\n",
       "                      consultation_doc_ref mapped_DIAGNOSIS mapped_COMPLAINT  \n",
       "0                                      NaN              NaN              NaN  \n",
       "1                                      NaN              NaN              NaN  \n",
       "2  1653486348_pdf_Psn4t6MIZJAFW3uYoR7e.pdf              htn              NaN  \n",
       "3                                      NaN              NaN              NaN  \n",
       "4  1646656625_pdf_t9oIOr8FNXGkmLnE245S.pdf       pityriasis              NaN  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

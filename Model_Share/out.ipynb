{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from negspacy.negation import Negex\n",
    "from scispacy.linking import EntityLinker\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "from memory_profiler import profile\n",
    "import time\n",
    "import sys\n",
    "import psutil \n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "#extractor_to_use = 1 Use en_core_sci_lg model - Link to UMLS to extract complaint,disease, drugs etc\n",
    "#extractor_to_use = 2 Use en_ner_bc5cdr_md model\n",
    "#extractor_to_use = 3 Use a combination of model 1 and 2\n",
    "#extractor_to_use = 4 Use unigram,bigram,trigram based matching models\n",
    "#extractor_to_use = 5 Use a combination of extractors 1 and 4\n",
    "#extractor_to_use = 6 Use a combination of extractors 2 and 4\n",
    "#extractor_to_use = 7 Use a combination of extractors 3 and 4\n",
    "extractor_to_use = 5\n",
    "# Functions needed for entity extraction models\n",
    "f=open('acronyms.txt','r')\n",
    "acronyms_dict={}\n",
    "l=f.readline()\n",
    "l=f.readline()\n",
    "while l:\n",
    "    l1=l.split(\"\\t\")\n",
    "    acronyms_dict[l1[0].lower()]=l1[1].lower()\n",
    "    l=f.readline()\n",
    "f.close()\n",
    "\n",
    "\n",
    "#lemmatizing the notes to capture all forms of negation(e.g., deny: denies, denying)\n",
    "def lemmatize(note, nlp):\n",
    "    doc = nlp(note)\n",
    "    lemNote = [wd.lemma_ for wd in doc]\n",
    "    return \" \".join(lemNote)\n",
    "\n",
    "def acronym_to_full_text(row,acronyms_dict):\n",
    "    final_text = \"\"\n",
    "  \n",
    "    for word in row.split(\" \"):\n",
    "        if word.strip() in acronyms_dict.keys():\n",
    "            final_text=final_text+acronyms_dict[word.strip()]+\" \"\n",
    "        else:\n",
    "            final_text=final_text+word.strip()+\" \"\n",
    "    return final_text.strip()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def preprocessing_matching_algorithm():\n",
    "    Full_data=pd.ExcelFile(r'Diagnosis N Gram MAX 3 12April22upd2.xlsx',engine='openpyxl')\n",
    "\n",
    "    one_gram=pd.read_excel(Full_data,sheet_name='Unigram')\n",
    "\n",
    "    #del one_gram['Unnamed: 5']\n",
    "    one_gram=one_gram.dropna()\n",
    "    one_gram.rename(columns={'Category(Investigation/Complaint/Diagnosis)':'Category(Investigation/Complaint/Diagnosis/Procedure)'},inplace=True)\n",
    "    one_gram.dropna(subset=['Label'],inplace=True)\n",
    "    one_gram=one_gram[['word','Category(Investigation/Complaint/Diagnosis/Procedure)','Label']]\n",
    "\n",
    "\n",
    "    Bigram=pd.read_excel(Full_data,sheet_name='Bigram')\n",
    "    Bigram['Label']=np.where(Bigram['Label2'].isna(),Bigram['Label1'],Bigram['Label1']+\" \"+Bigram['Label2'])\n",
    "    Bigram.rename(columns={'Category(Investigation/Complaint/Diagnosis)':'Category(Investigation/Complaint/Diagnosis/Procedure)'},inplace=True)\n",
    "    Bigram.dropna(subset=['Label'],inplace=True)\n",
    "    Bigram=Bigram[['word','Category(Investigation/Complaint/Diagnosis/Procedure)','Label']]\n",
    "\n",
    "    \n",
    "    Trigram=pd.read_excel(Full_data,sheet_name='Trigram')\n",
    "    Trigram.fillna(\"\",inplace=True)\n",
    "    Trigram['Label']=Trigram['Label1']+\" \"+Trigram['Label2']+\" \"+Trigram['Label3']\n",
    "    Trigram['Label']=Trigram['Label'].str.strip()\n",
    "    Trigram['len']=Trigram.Label.str.len()\n",
    "    Trigram=Trigram[Trigram['len']>0]\n",
    "    Trigram.dropna(subset=['Label'],inplace=True)\n",
    "    Trigram=Trigram[['word','Category(Investigation/Complaint/Diagnosis/Procedure)','Label']]\n",
    "    \n",
    "    n_gram_sheet=one_gram.append(Bigram)\n",
    "    n_gram_sheet=n_gram_sheet.append(Trigram)\n",
    "    n_gram_sheet.fillna(\"\",inplace=True)\n",
    "    n_gram_sheet=n_gram_sheet.replace('nan',\"\")\n",
    "    n_gram_sheet['len']=n_gram_sheet['Category(Investigation/Complaint/Diagnosis/Procedure)'].str.len()\n",
    "    n_gram_sheet=n_gram_sheet[n_gram_sheet['len']>0]\n",
    "    n_gram_sheet['label']=n_gram_sheet['Category(Investigation/Complaint/Diagnosis/Procedure)']+\":\"+n_gram_sheet['Label']\n",
    "    n_gram_sheet.rename(columns={'word':'pattern'},inplace=True)\n",
    "    \n",
    "    # investigation=pd.read_excel('UnifiedBiomarkerMapping_1.xlsx',sheet_name='Top TestNames')\n",
    "    # investigation.dropna()\n",
    "    # investigation['x'] = pd.to_numeric(investigation['lab test names'], errors='coerce')\n",
    "    # investigation = investigation[investigation['x'].isnull()]\n",
    "    # investigation=investigation[['lab test names']]\n",
    "    # investigation['lab test names']= investigation['lab test names'].apply(lambda x:''.join(re.sub(r'\\W+',' ',x)))\n",
    "    # investigation['lab test names']= investigation['lab test names'].apply(lambda x:re.sub(' {2,}', ' ',x))\n",
    "    # investigation['lab test names']=investigation['lab test names'].astype(str)\n",
    "    # investigation['len']=investigation['lab test names'].apply(lambda x:len(x))\n",
    "    # investigation.drop_duplicates(inplace=True)\n",
    "    # investigation.rename(columns={'lab test names':'pattern'},inplace=True)\n",
    "    # investigation['pattern']=investigation['pattern'].astype(str)\n",
    "    # investigation['Label']='Investigation:'+investigation['pattern']\n",
    "\n",
    "    \n",
    "    investigation=pd.read_excel('UnifiedBiomarkerMapping_1.xlsx',engine='openpyxl',sheet_name='Top TestNames')\n",
    "    investigation.dropna()\n",
    "    investigation.rename(columns={'lab test names':'pattern'},inplace=True)\n",
    "    investigation['pattern']=investigation['pattern'].astype(str)\n",
    "\n",
    "    investigation['label']='Investigation:'+(investigation['pattern'])\n",
    "    n_gram_sheet=n_gram_sheet.append(investigation)\n",
    "\n",
    "    drugs=pd.read_csv('final_drug_list.csv')\n",
    "    drugs.dropna()\n",
    "    drugs.rename(columns={'value_upd_3':'pattern'},inplace=True)\n",
    "    drugs['label']='Drugs:'+drugs['pattern']\n",
    "   # print(n_gram_sheet)\n",
    "    n_gram_sheet=n_gram_sheet.append(drugs)\n",
    "    n_gram_sheet=n_gram_sheet[['label','pattern']]\n",
    "\n",
    "    gram_dict=n_gram_sheet.to_dict(\"record\")\n",
    "    #print(gram_dict)\n",
    "    length_gram_dict = len(gram_dict)\n",
    "    for i in range(0,length_gram_dict):\n",
    "        pattern = gram_dict[i]['pattern']\n",
    "        pattern = acronym_to_full_text(pattern,acronyms_dict)\n",
    "        pattern_list = pattern.split()\n",
    "        l=[]\n",
    "        l1 = []\n",
    "        for j in range(0,len(pattern_list)):\n",
    "            d={}\n",
    "            d[\"LOWER\"]=pattern_list[j]\n",
    "            l.append(d)\n",
    "            l1.append(d)\n",
    "            if j!=len(pattern_list)-1:\n",
    "                l1.append({\"IS_PUNCT\": True})\n",
    "        gram_dict.append({}) \n",
    "        gram_dict[i]['pattern'] = l\n",
    "        \n",
    "        gram_dict[i+length_gram_dict]['pattern'] = l1\n",
    "        gram_dict[i+length_gram_dict]['label'] = gram_dict[i]['label']\n",
    "    #print(gram_dict)\n",
    "    return gram_dict\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#Reference https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt\n",
    "valid_semantic_types = ['T109','T116','T020','T190','T017','T195','T053','T038','T030','T019','T200','T060','T047','T129','T037','T059','T034','T048','T046','T121','T061','T184','T033','T191' ]\n",
    "complaint_semantic_types = ['T184','T033']\n",
    "diagnosis_semantic_types = ['T020','T190','T053','T038','T029','T023','T030','T019','T047','T129','T037','T048','T046','T191']\n",
    "investigation_semantic_types=['T017','T034','T060','T059','T061']\n",
    "procedure_semantic_types=[]\n",
    "drugs_semantic_types = ['T195','T200','T116','T109','T122']\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "patterns = preprocessing_matching_algorithm()\n",
    "\n",
    "if extractor_to_use==1 or extractor_to_use == 3:\n",
    "    nlp1 = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "    nlp1.add_pipe(\"scispacy_linker\", config={\"linker_name\": \"umls\",\"max_entities_per_mention\": 1,\"threshold\": 0.9})\n",
    "    nlp1.add_pipe(\"negex\",config={\"chunk_prefix\": [\"no\"]})\n",
    "if extractor_to_use == 2 or extractor_to_use == 3:\n",
    "    nlp2 = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "    nlp2.add_pipe(\"negex\",config={\"chunk_prefix\": [\"no\"]})\n",
    "if extractor_to_use == 4:\n",
    "    nlp3 = English()\n",
    "    ruler = nlp3.add_pipe(\"entity_ruler\", config={\"overwrite_ents\":True})\n",
    "    ruler.add_patterns(patterns)\n",
    "    nlp3.add_pipe(\"negex\",config={\"chunk_prefix\": [\"no\"]})\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "if extractor_to_use == 5 or extractor_to_use == 7:\n",
    "    nlp4 = spacy.load(\"en_core_sci_lg\")\n",
    "    nlp4.add_pipe(\"scispacy_linker\", config={\"linker_name\": \"umls\",\"max_entities_per_mention\": 1,\"threshold\": 0.9})\n",
    "    ruler1 = nlp4.add_pipe(\"entity_ruler\", config={\"overwrite_ents\":True})\n",
    "    ruler1.add_patterns(patterns)\n",
    "    nlp4.add_pipe(\"negex\",config={\"chunk_prefix\": [\"no\"]})\n",
    "  \n",
    "\n",
    "if extractor_to_use == 6 or extractor_to_use == 7:\n",
    "    nlp5 = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "    ruler2 = nlp5.add_pipe(\"entity_ruler\", config={\"overwrite_ents\":True})\n",
    "    ruler2.add_patterns(patterns)\n",
    "    nlp5.add_pipe(\"negex\",config={\"chunk_prefix\": [\"no\"]})\n",
    "    \n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "\n",
    "def entity_extraction(row,value):\n",
    " #   print(row[value])\n",
    "    val_list =row[value].split(\"##\")\n",
    "   # print(val_list)\n",
    "    complaint = []\n",
    "    diagnosis = []\n",
    "    investigation=[]\n",
    "    procedure = []\n",
    "    drugs=[]\n",
    "        \n",
    "    for val in val_list:\n",
    "        regexpattern = r'\\.\\.+'\n",
    "        val = re.sub(regexpattern, ' ', val)\n",
    "        val  = ' '.join(acronym_to_full_text(val.strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split())\n",
    "\n",
    "        #print(\"val is \",val)\n",
    "            \n",
    "        if extractor_to_use==1 or extractor_to_use == 3:\n",
    "            doc1 = nlp1(val)\n",
    "        if extractor_to_use==2 or extractor_to_use ==3 :\n",
    "            doc2 = nlp2(val)\n",
    "        if extractor_to_use==4:\n",
    "            doc3 = nlp3(val)\n",
    "        if extractor_to_use==5 or extractor_to_use==7:\n",
    "            doc4 = nlp4(val)\n",
    "        if extractor_to_use==6 or extractor_to_use==7:\n",
    "            doc5=nlp5(val)\n",
    "        \n",
    "       \n",
    "        if extractor_to_use == 4 or extractor_to_use==5 or extractor_to_use==6 or extractor_to_use==7:\n",
    "            \n",
    "            label=[]\n",
    "            text=[]\n",
    "            ent_list=[]\n",
    "            if extractor_to_use ==4:\n",
    "                ent_list.append(doc3.ents)\n",
    "            if extractor_to_use == 5 or extractor_to_use==7:\n",
    "                ent_list.append(doc4.ents)\n",
    "            if extractor_to_use==6 or extractor_to_use == 7:\n",
    "                ent_list.append(doc5.ents)\n",
    "            \n",
    "            for ent_tuple in ent_list:\n",
    "                for ent in ent_tuple:\n",
    "                   # print(ent,ent.label_,ent.text,ent._.negex)\n",
    "                    if ent._.negex == 0 and \":\" in ent.label_:\n",
    "                        label.append(str(ent.label_))\n",
    "                        text.append(ent.text)\n",
    "            \n",
    "            if len(label) > 0:\n",
    "                all_labels=pd.DataFrame(label,columns =['Names'])\n",
    "                all_labels[['Category', 'insight']] = all_labels['Names'].str.split(':', 1, expand=True)\n",
    "                all_labels['Category']=all_labels.Category.str.strip()\n",
    "                all_labels['insight']=all_labels.insight.str.strip()\n",
    "                final_label=all_labels[['Category','insight']].groupby(['Category'])['insight'].apply(','.join).reset_index()\n",
    "                #print(final_label)\n",
    "                for i in range(len(final_label)):    \n",
    "                    if final_label.iloc[i]['Category']=='Complaint':\n",
    "                        if final_label.iloc[i]['insight'] not in complaint:\n",
    "                            complaint.append(final_label.iloc[i]['insight'])\n",
    "                    elif final_label.iloc[i]['Category']=='Investigation':\n",
    "                        if final_label.iloc[i]['insight'] not in investigation:\n",
    "                            investigation.append(final_label.iloc[i]['insight'])\n",
    "                    elif final_label.iloc[i]['Category']=='Diagnosis':\n",
    "                        if final_label.iloc[i]['insight'] not in diagnosis:\n",
    "                            diagnosis.append(final_label.iloc[i]['insight'])\n",
    "                    elif final_label.iloc[i]['Category']=='Drugs':\n",
    "                        if final_label.iloc[i]['insight'] not in drugs:\n",
    "                            drugs.append(final_label.iloc[i]['insight'])\n",
    "                    elif final_label.iloc[i]['Category']=='Procedure':\n",
    "                        if final_label.iloc[i]['insight'] not in procedure:\n",
    "                            procedure.append(final_label.iloc[i]['insight']) \n",
    "          \n",
    "        if extractor_to_use ==1 or extractor_to_use == 3 or extractor_to_use == 5 or extractor_to_use==7:\n",
    "            if extractor_to_use ==1 or extractor_to_use == 3:\n",
    "                ent_list = doc1.ents\n",
    "                linker = nlp1.get_pipe(\"scispacy_linker\")\n",
    "            else:\n",
    "                ent_list = doc4.ents\n",
    "                linker = nlp4.get_pipe(\"scispacy_linker\")\n",
    "            #print(ent_list)\n",
    "            for entity in ent_list:\n",
    "                #print(entity.text,entity.label_,entity._.negex,entity._.kb_ents)\n",
    "                if entity._.negex == 0 and entity.label_=='ENTITY':\n",
    "                #Link to UMLS knowledge baseT048\n",
    "                \n",
    "             #   print(entity._.kb_ents)\n",
    "                    for kb_entry in entity._.kb_ents:\n",
    "                        sem_type=linker.kb.cui_to_entity[kb_entry[0]].types[0]\n",
    "                        #print(entity,sem_type,linker.kb.cui_to_entity[kb_entry[0]].types)\n",
    "                        #print(linker.kb.cui_to_entity[kb_entry[0]].canonical_name)\n",
    "                        if sem_type in valid_semantic_types:\n",
    "                            if sem_type in complaint_semantic_types:\n",
    "                                if str(entity.text) not in complaint:\n",
    "                                    complaint.append(str(entity.text))\n",
    "                            elif sem_type in diagnosis_semantic_types:\n",
    "                                if str(entity.text) not in diagnosis:\n",
    "                                    diagnosis.append(str(entity.text))\n",
    "                            elif sem_type in investigation_semantic_types:\n",
    "                                if str(entity.text) not in investigation:\n",
    "                                    investigation.append(str(entity.text))\n",
    "                            elif sem_type in procedure_semantic_types:\n",
    "                                if str(entity.text) not in procedure:\n",
    "                                    procedure.append(str(entity.text))\n",
    "                            elif sem_type in drugs_semantic_types:\n",
    "                                if str(entity.text) not in drugs:\n",
    "                                    drugs.append(str(entity.text))\n",
    "            \n",
    "                \n",
    "        if extractor_to_use==2 or extractor_to_use ==3 or extractor_to_use==6 or extractor_to_use==7:\n",
    "            if extractor_to_use == 2 or extractor_to_use == 3:\n",
    "                ent_list = doc2.ents\n",
    "            else:\n",
    "                ent_list = doc5.ents\n",
    "            for entity in ent_list:\n",
    "                if entity._.negex == 0:\n",
    "                    if entity.label_ == 'DISEASE':\n",
    "                        if str(entity.text) not in diagnosis and str(entity.text) not in complaint:\n",
    "                            diagnosis.append(str(entity.text))\n",
    "                    elif entity.label_ == 'CHEMICAL':\n",
    "                        if str(entity.text) not in drugs:\n",
    "                            drugs.append(str(entity.text))\n",
    "        \n",
    "       \n",
    "                    \n",
    "     \n",
    "       \n",
    "    row['COMPLAINT'] = \",\".join(complaint)\n",
    "    row['DIAGNOSIS'] = \",\".join(diagnosis)\n",
    "    row['INVESTIGATION'] = \",\".join(investigation)\n",
    "    row['PROCEDURE'] = \",\".join(procedure)\n",
    "    row['DRUGS'] = ','.join(drugs)\n",
    "    \n",
    "        \n",
    "    return row\n",
    "    \n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(row):\n",
    "   rowlist = row.split(\",\")\n",
    "   modrowlist=[]\n",
    "   for r in rowlist:\n",
    "       if r.strip() not in modrowlist and r.strip()!=\"\":\n",
    "           modrowlist.append(r.strip())\n",
    "   return \",\".join(modrowlist)\n",
    "     \n",
    "def len_distribution(df_ext_Remarks,df_ext_diagnosis,df_ext_Investigation):\n",
    "   df_ext=df_ext_Remarks.merge(df_ext_diagnosis,on=['Prescription id', 'Prescription_Diagnosis','Prescription_Investigation','Prescription_Medicine_Advised', 'Prescription_Remarks'],how='left')\n",
    "   print(df_ext.shape)\n",
    "   df_ext=df_ext.merge(df_ext_Investigation,on=['Prescription id', 'Prescription_Diagnosis','Prescription_Investigation', 'Prescription_Medicine_Advised','Prescription_Remarks'],how='left')\n",
    "   print(df_ext.shape)\n",
    "   df_ext=df_ext.replace('nan',\"\")\n",
    "   df_ext['COMPLAINT_upd']=df_ext['COMPLAINT']+\",\"+df_ext['COMPLAINT_x']+\",\"+df_ext['COMPLAINT_y']\n",
    "   df_ext['DIAGNOSIS_upd']=df_ext['DIAGNOSIS']+\",\"+df_ext['DIAGNOSIS_x']+\",\"+df_ext['DIAGNOSIS_y']\n",
    "   df_ext['INVESTIGATION_upd']=df_ext['INVESTIGATION']+\",\"+df_ext['INVESTIGATION_x']+\",\"+df_ext['INVESTIGATION_y']\n",
    "   df_ext['PROCEDURE_upd']=df_ext['PROCEDURE']+\",\"+df_ext['PROCEDURE_x']+\",\"+df_ext['PROCEDURE_y']\n",
    "   df_ext['DRUGS_upd']=df_ext['DRUGS']+\",\"+df_ext['DRUGS_x']+\",\"+df_ext['DRUGS_y']\n",
    "  \n",
    "   df_ext['COMPLAINT_upd'] = df_ext.apply(lambda row : clean_data(row['COMPLAINT_upd'].lower()), axis = 1)\n",
    "   df_ext['DIAGNOSIS_upd'] = df_ext.apply(lambda row : clean_data(row['DIAGNOSIS_upd'].lower()), axis = 1)\n",
    "   df_ext['INVESTIGATION_upd'] = df_ext.apply(lambda row : clean_data(row['INVESTIGATION_upd'].lower()), axis = 1)\n",
    "   df_ext['PROCEDURE_upd'] = df_ext.apply(lambda row : clean_data(row['PROCEDURE_upd'].lower()), axis = 1)\n",
    "   df_ext['DRUGS_upd'] = df_ext.apply(lambda row : clean_data(row['DRUGS_upd'].lower()), axis = 1)\n",
    "   \n",
    "   df_ext.fillna(\"\",inplace=True)\n",
    "   df_ext=df_ext.replace('nan',\"\")\n",
    "\n",
    "\n",
    "   x=df_ext[['Prescription id', 'Prescription_Diagnosis','Prescription_Investigation', 'Prescription_Medicine_Advised','Prescription_Remarks','COMPLAINT_upd', 'DIAGNOSIS_upd', 'INVESTIGATION_upd','DRUGS_upd','PROCEDURE_upd']]   \n",
    "   x['length']=x['COMPLAINT_upd'].apply(lambda y:len(str(y))) \n",
    "   x['length1']=x['DIAGNOSIS_upd'].apply(lambda y:len(str(y)))\n",
    "   x['length2']=x['DRUGS_upd'].apply(lambda y:len(str(y)))\n",
    "   x['length3']=x['INVESTIGATION_upd'].apply(lambda y:len(str(y)))\n",
    "   x['length4']=x['PROCEDURE_upd'].apply(lambda y:len(str(y)))\n",
    "  \n",
    "   x['len']=x['length']+x['length1']+x['length2']+x['length3']+x['length4']\n",
    "   return x\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "data=pd.read_excel('Validation_Dataset_10.xlsx',engine='openpyxl',usecols='A,B,C,D,E')\n",
    "print(data.shape)\n",
    "data.dropna(how='all',axis=0,inplace=True)\n",
    "#data.dropna(how='all', axis=1, inplace=True)\n",
    "print(data.shape)\n",
    "data['Prescription_Remarks']=data['Prescription_Remarks'].astype(str)\n",
    "data['Prescription_Diagnosis']=data['Prescription_Diagnosis'].astype(str)\n",
    "data['Prescription_Investigation']=data['Prescription_Investigation'].astype(str)\n",
    "data['Prescription_Medicine_Advised']=data['Prescription_Medicine_Advised'].astype(str)\n",
    "nan_value = float(\"NaN\")\n",
    "data.replace(\"\", nan_value, inplace=True)\n",
    "print(data.shape)\n",
    "data.dropna(how='all',axis=0,inplace=True)\n",
    "data.dropna(how='all', axis=1, inplace=True)\n",
    "print(data.shape)\n",
    "df_ext_Remarks=data.apply(lambda row:entity_extraction(row,'Prescription_Remarks'),axis=1)\n",
    "print(\"here1\")\n",
    "df_ext_diagnosis=data.apply(lambda row:entity_extraction(row,'Prescription_Diagnosis'),axis=1)\n",
    "print(\"here2\")\n",
    "df_ext_Investigation=data.apply(lambda row:entity_extraction(row,'Prescription_Investigation'),axis=1)\n",
    "\n",
    "print(\"here3\")\n",
    "df_ext_medicine=data.apply(lambda row:entity_extraction(row,'Prescription_Medicine_Advised'),axis=1)\n",
    "\n",
    "\n",
    "print(\"here4\")\n",
    "df_ext_Remarks.to_csv(\"1.csv\")\n",
    "df_ext_diagnosis.to_csv(\"2.csv\")\n",
    "df_ext_Investigation.to_csv(\"3.csv\")\n",
    "print(df_ext_Remarks.shape)\n",
    "print(df_ext_diagnosis.shape)\n",
    "print(df_ext_Investigation.shape)\n",
    "\n",
    "df_extractor_output=len_distribution(df_ext_Remarks,df_ext_diagnosis,df_ext_Investigation)\n",
    "\n",
    "df_extractor_output.to_csv('entity_extractor_model_'+str(extractor_to_use)+'_output_test4.csv')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Generate Results:\n",
    "\n",
    "def generate_result_complaint(row):\n",
    "    fully_matched_complaint = 0\n",
    "    false_positive_complaint = 0\n",
    "    false_negative_complaint = 0\n",
    "    partial_overlap_complaint = 0\n",
    "    NUM_COMPLAINTS = 5\n",
    "    #print(row['COMPLAINT_upd'])\n",
    "    complaint_list = row['COMPLAINT_upd'].split(\",\")\n",
    "    complaint_list = [i for i in complaint_list if i]\n",
    "    complaint1_tokens = acronym_to_full_text(row['Complaint1'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    complaint2_tokens = acronym_to_full_text(row['Complaint2'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    complaint3_tokens = acronym_to_full_text(row['Complaint3'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    complaint4_tokens = acronym_to_full_text(row['Complaint4'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    complaint5_tokens = acronym_to_full_text(row['Complaint5'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    complaint1_tokens = [i for i in complaint1_tokens if i]\n",
    "    complaint2_tokens = [i for i in complaint2_tokens if i]\n",
    "    complaint3_tokens = [i for i in complaint3_tokens if i]\n",
    "    complaint4_tokens = [i for i in complaint4_tokens if i]\n",
    "    complaint5_tokens = [i for i in complaint5_tokens if i]\n",
    "    complaint_tokens = []\n",
    "    complaint_tokens.append(complaint1_tokens)\n",
    "    complaint_tokens.append(complaint2_tokens)\n",
    "    complaint_tokens.append(complaint3_tokens)\n",
    "    complaint_tokens.append(complaint4_tokens)\n",
    "    complaint_tokens.append(complaint5_tokens)\n",
    "    complaints_matched=[]\n",
    "   \n",
    "    for i in range(NUM_COMPLAINTS):\n",
    "        if len(complaint_tokens[i]) > 0 and complaint_tokens[i][0]!=\"\":\n",
    "            complaints_matched.append(0)\n",
    "        else:\n",
    "            complaints_matched.append(2)\n",
    "    \n",
    "    for comp in complaint_list:\n",
    "        comp_tokens = acronym_to_full_text(comp.strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "        tot_tokens_comp_alg = len(comp_tokens)\n",
    "        #Change this line if NUM_COMPLAINTS change\n",
    "        tot_tokens_comp_doc= [len(complaint1_tokens),len(complaint2_tokens),len(complaint3_tokens),len(complaint4_tokens),len(complaint5_tokens)]\n",
    "        count = 0\n",
    "        for i in range(NUM_COMPLAINTS):\n",
    "            for x in comp_tokens:\n",
    "                if complaints_matched[i] == 0:\n",
    "                    if x in complaint_tokens[i]:\n",
    "                        count+=1\n",
    "            if count > 0 and count == tot_tokens_comp_doc[i] and count==tot_tokens_comp_alg:\n",
    "                fully_matched_complaint+=1\n",
    "                complaints_matched[i] = 1\n",
    "                break\n",
    "            elif count > 0:\n",
    "                complaints_matched[i] = 1\n",
    "                partial_overlap_complaint+=1;\n",
    "                break\n",
    "        if count==0:\n",
    "            false_positive_complaint+=1\n",
    "                 \n",
    "    for i in range(NUM_COMPLAINTS):\n",
    "        if complaints_matched[i] == 0:\n",
    "            false_negative_complaint+=1\n",
    "        \n",
    "    row['Complaint_TP']=fully_matched_complaint\n",
    "    row['Complaint_FP']=false_positive_complaint\n",
    "    row['Complaint_FN']=false_negative_complaint\n",
    "    row['Complaint_Paritial_Matched']=partial_overlap_complaint\n",
    "    return row\n",
    "\n",
    "def generate_result_diagnosis(row):\n",
    "    fully_matched_diagnosis = 0\n",
    "    false_positive_diagnosis = 0\n",
    "    false_negative_diagnosis = 0\n",
    "    partial_overlap_diagnosis = 0\n",
    "    NUM_DIAGNOSIS = 5\n",
    "    #print(row['DIAGNOSIS_upd'])\n",
    "    diagnosis_list = row['DIAGNOSIS_upd'].split(\",\")\n",
    "    diagnosis_list = [i for i in diagnosis_list if i]\n",
    "    diagnosis1_tokens = acronym_to_full_text(row['Diagnosis1'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    diagnosis2_tokens = acronym_to_full_text(row['Diagnosis2'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    diagnosis3_tokens = acronym_to_full_text(row['Diagnosis3'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    diagnosis4_tokens = acronym_to_full_text(row['Diagnosis4'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    diagnosis5_tokens = acronym_to_full_text(row['Diagnosis5'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    diagnosis1_tokens = [i for i in diagnosis1_tokens if i]\n",
    "    diagnosis2_tokens = [i for i in diagnosis2_tokens if i]\n",
    "    diagnosis3_tokens = [i for i in diagnosis3_tokens if i]\n",
    "    diagnosis4_tokens = [i for i in diagnosis4_tokens if i]\n",
    "    diagnosis5_tokens = [i for i in diagnosis5_tokens if i]\n",
    "    diagnosis_tokens = []\n",
    "    diagnosis_tokens.append(diagnosis1_tokens)\n",
    "    diagnosis_tokens.append(diagnosis2_tokens)\n",
    "    diagnosis_tokens.append(diagnosis3_tokens)\n",
    "    diagnosis_tokens.append(diagnosis4_tokens)\n",
    "    diagnosis_tokens.append(diagnosis5_tokens)\n",
    "    diagnosis_matched=[]\n",
    "   \n",
    "    for i in range(NUM_DIAGNOSIS):\n",
    "        if len(diagnosis_tokens[i]) > 0 and diagnosis_tokens[i][0]!=\"\":\n",
    "            diagnosis_matched.append(0)\n",
    "        else:\n",
    "            diagnosis_matched.append(2)\n",
    "    \n",
    "    for diag in diagnosis_list:\n",
    "        diag_tokens = acronym_to_full_text(diag.strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "        tot_tokens_diag_alg = len(diag_tokens)\n",
    "        #Change this line if NUM_DIAGNOSIS change\n",
    "        tot_tokens_diag_doc = [len(diagnosis1_tokens),len(diagnosis2_tokens),len(diagnosis3_tokens),len(diagnosis4_tokens),len(diagnosis5_tokens)]\n",
    "        count = 0\n",
    "        for i in range(NUM_DIAGNOSIS):\n",
    "            for x in diag_tokens:\n",
    "                if diagnosis_matched[i] == 0:\n",
    "                    if x in diagnosis_tokens[i]:\n",
    "                        count+=1\n",
    "            if count > 0 and count == tot_tokens_diag_doc[i] and count==tot_tokens_diag_alg:\n",
    "                fully_matched_diagnosis+=1\n",
    "                diagnosis_matched[i] = 1\n",
    "                break\n",
    "            elif count > 0:\n",
    "                diagnosis_matched[i] = 1\n",
    "                partial_overlap_diagnosis+=1;\n",
    "                break\n",
    "        if count==0:\n",
    "            false_positive_diagnosis+=1\n",
    "                 \n",
    "    for i in range(NUM_DIAGNOSIS):\n",
    "        if diagnosis_matched[i] == 0:\n",
    "            false_negative_diagnosis+=1\n",
    "        \n",
    "    row['Diagnosis_TP']=fully_matched_diagnosis\n",
    "    row['Diagnosis_FP']=false_positive_diagnosis\n",
    "    row['Diagnosis_FN']=false_negative_diagnosis\n",
    "    row['Diagnosis_Paritial_Matched']=partial_overlap_diagnosis\n",
    "    return row\n",
    "\n",
    "def generate_result_investigation(row):\n",
    "    fully_matched_investigation = 0\n",
    "    false_positive_investigation = 0\n",
    "    false_negative_investigation = 0\n",
    "    partial_overlap_investigation = 0\n",
    "    NUM_INVESTIGATION = 5\n",
    "   # print(row['INVESTIGATION_upd'])\n",
    "    investigation_list = row['INVESTIGATION_upd'].split(\",\")\n",
    "    investigation_list = [i for i in investigation_list if i]\n",
    "    investigation1_tokens = acronym_to_full_text(row['Investigation1'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    investigation2_tokens = acronym_to_full_text(row['Investigation2'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    investigation3_tokens = acronym_to_full_text(row['Investigation3'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    investigation4_tokens = acronym_to_full_text(row['Investigation4'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    investigation5_tokens = acronym_to_full_text(row['Investigation5'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    investigation1_tokens = [i for i in investigation1_tokens if i]\n",
    "    investigation2_tokens = [i for i in investigation2_tokens if i]\n",
    "    investigation3_tokens = [i for i in investigation3_tokens if i]\n",
    "    investigation4_tokens = [i for i in investigation4_tokens if i]\n",
    "    investigation5_tokens = [i for i in investigation5_tokens if i]\n",
    "    investigation_tokens = []\n",
    "    investigation_tokens.append(investigation1_tokens)\n",
    "    investigation_tokens.append(investigation2_tokens)\n",
    "    investigation_tokens.append(investigation3_tokens)\n",
    "    investigation_tokens.append(investigation4_tokens)\n",
    "    investigation_tokens.append(investigation5_tokens)\n",
    "    investigation_matched=[]\n",
    "   \n",
    "    for i in range(NUM_INVESTIGATION):\n",
    "        if len(investigation_tokens[i]) > 0 and investigation_tokens[i][0]!=\"\":\n",
    "            investigation_matched.append(0)\n",
    "        else:\n",
    "            investigation_matched.append(2)\n",
    "    \n",
    "    for invest in investigation_list:\n",
    "        invest_tokens = acronym_to_full_text(invest.strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "        tot_tokens_invest_alg = len(invest_tokens)\n",
    "        #Change this line if NUM_INVESTIGATION change\n",
    "        tot_tokens_invest_doc = [len(investigation1_tokens),len(investigation2_tokens),len(investigation3_tokens),len(investigation4_tokens),len(investigation5_tokens)]\n",
    "        count = 0\n",
    "        for i in range(NUM_INVESTIGATION):\n",
    "            for x in invest_tokens:\n",
    "                if investigation_matched[i] == 0:\n",
    "                    if x in investigation_tokens[i]:\n",
    "                        count+=1\n",
    "            if count > 0 and count == tot_tokens_invest_doc[i] and count==tot_tokens_invest_alg:\n",
    "                fully_matched_investigation+=1\n",
    "                investigation_matched[i] = 1\n",
    "                break\n",
    "            elif count > 0:\n",
    "                investigation_matched[i] = 1\n",
    "                partial_overlap_investigation+=1;\n",
    "                break\n",
    "        if count==0:\n",
    "            false_positive_investigation+=1\n",
    "                 \n",
    "    for i in range(NUM_INVESTIGATION):\n",
    "        if investigation_matched[i] == 0:\n",
    "            false_negative_investigation+=1\n",
    "        \n",
    "    row['Investigation_TP']=fully_matched_investigation\n",
    "    row['Investigation_FP']=false_positive_investigation\n",
    "    row['Investigation_FN']=false_negative_investigation\n",
    "    row['Investigation_Paritial_Matched']=partial_overlap_investigation\n",
    "    return row\n",
    "\n",
    "def generate_result_procedure(row):\n",
    "    fully_matched_procedure = 0\n",
    "    false_positive_procedure = 0\n",
    "    false_negative_procedure = 0\n",
    "    partial_overlap_procedure = 0\n",
    "    NUM_PROCEDURE = 2\n",
    "  #  print(row['PROCEDURE_upd'])\n",
    "    procedure_list = row['PROCEDURE_upd'].split(\",\")\n",
    "    procedure_list = [i for i in procedure_list if i]\n",
    "    procedure1_tokens = acronym_to_full_text(row['Procedure1'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    procedure2_tokens = acronym_to_full_text(row['Procedure2'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    # procedure3_tokens = acronym_to_full_text(row['Procedure3'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    procedure1_tokens = [i for i in procedure1_tokens if i]\n",
    "    procedure2_tokens = [i for i in procedure2_tokens if i]\n",
    "    # procedure3_tokens = [i for i in procedure3_tokens if i]\n",
    "    procedure_tokens = []\n",
    "    procedure_tokens.append(procedure1_tokens)\n",
    "    procedure_tokens.append(procedure2_tokens)\n",
    "    # procedure_tokens.append(procedure3_tokens)\n",
    "    procedure_matched=[]\n",
    "   \n",
    "    for i in range(NUM_PROCEDURE):\n",
    "        if len(procedure_tokens[i]) > 0 and procedure_tokens[i][0]!=\"\":\n",
    "            procedure_matched.append(0)\n",
    "        else:\n",
    "            procedure_matched.append(2)\n",
    "    \n",
    "    for proc in procedure_list:\n",
    "        proc_tokens = acronym_to_full_text(proc.strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "        tot_tokens_proc_alg = len(proc_tokens)\n",
    "        #Change this line if NUM_PROCEDURE change\n",
    "        # tot_tokens_proc_doc = [len(procedure1_tokens),len(procedure2_tokens),len(procedure3_tokens)]\n",
    "        tot_tokens_proc_doc = [len(procedure1_tokens),len(procedure2_tokens)]\n",
    "\n",
    "        count = 0\n",
    "        for i in range(NUM_PROCEDURE):\n",
    "            for x in proc_tokens:\n",
    "                if procedure_matched[i] == 0:\n",
    "                    if x in procedure_tokens[i]:\n",
    "                        count+=1\n",
    "            if count > 0 and count == tot_tokens_proc_doc[i] and count==tot_tokens_proc_alg:\n",
    "                fully_matched_procedure+=1\n",
    "                procedure_matched[i] = 1\n",
    "                break\n",
    "            elif count > 0:\n",
    "                procedure_matched[i] = 1\n",
    "                partial_overlap_procedure+=1;\n",
    "                break\n",
    "        if count==0:\n",
    "            false_positive_procedure+=1\n",
    "                 \n",
    "    for i in range(NUM_PROCEDURE):\n",
    "        if procedure_matched[i] == 0:\n",
    "            false_negative_procedure+=1\n",
    "        \n",
    "    row['Procedure_TP']=fully_matched_procedure\n",
    "    row['Procedure_FP']=false_positive_procedure\n",
    "    row['Procedure_FN']=false_negative_procedure\n",
    "    row['Procedure_Paritial_Matched']=partial_overlap_procedure\n",
    "    return row\n",
    "\n",
    "def generate_result_drugs(row):\n",
    "    fully_matched_drug = 0\n",
    "    false_positive_drug = 0\n",
    "    false_negative_drug = 0\n",
    "    partial_overlap_drug = 0\n",
    "    NUM_DRUG = 4\n",
    "    #print(row['DRUGS_upd'])\n",
    "    drug_list = row['DRUGS_upd'].split(\",\")\n",
    "    drug_list = [i for i in drug_list if i]\n",
    "    drug1_tokens = acronym_to_full_text(row['Drug1'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    drug2_tokens = acronym_to_full_text(row['Drug2'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    drug3_tokens = acronym_to_full_text(row['Drug3'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    drug4_tokens = acronym_to_full_text(row['Drug4'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    #drug5_tokens = acronym_to_full_text(row['Drug5'].lower().strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "    drug1_tokens = [i for i in drug1_tokens if i]\n",
    "    drug2_tokens = [i for i in drug2_tokens if i]\n",
    "    drug3_tokens = [i for i in drug3_tokens if i]\n",
    "    drug4_tokens = [i for i in drug4_tokens if i]\n",
    "    #drug5_tokens = [i for i in drug5_tokens if i]\n",
    "    drug_tokens = []\n",
    "    drug_tokens.append(drug1_tokens)\n",
    "    drug_tokens.append(drug2_tokens)\n",
    "    drug_tokens.append(drug3_tokens)\n",
    "    drug_tokens.append(drug4_tokens)\n",
    "    #drug_tokens.append(drug5_tokens)\n",
    "    drug_matched=[]\n",
    "   \n",
    "    for i in range(NUM_DRUG):\n",
    "        if len(drug_tokens[i]) > 0 and drug_tokens[i][0]!=\"\":\n",
    "            drug_matched.append(0)\n",
    "        else:\n",
    "            drug_matched.append(2)\n",
    "    \n",
    "    for drug in drug_list:\n",
    "        dr_tokens = acronym_to_full_text(drug.strip().replace('-',' ').replace('/',' ').replace('+',' ').strip(),acronyms_dict).split(\" \")\n",
    "        tot_tokens_drug_alg = len(dr_tokens)\n",
    "        #Change this line if NUM_DRUG change\n",
    "        tot_tokens_drug_doc = [len(drug1_tokens),len(drug2_tokens),len(drug3_tokens),len(drug4_tokens)]#,len(drug5_tokens)]\n",
    "        count = 0\n",
    "        for i in range(NUM_DRUG):\n",
    "            for x in dr_tokens:\n",
    "                if drug_matched[i] == 0:\n",
    "                    if x in drug_tokens[i]:\n",
    "                        count+=1\n",
    "            if count > 0 and count == tot_tokens_drug_doc[i] and count==tot_tokens_drug_alg:\n",
    "                fully_matched_drug+=1\n",
    "                drug_matched[i] = 1\n",
    "                break\n",
    "            elif count > 0:\n",
    "                drug_matched[i] = 1\n",
    "                partial_overlap_drug+=1;\n",
    "                break\n",
    "        if count==0:\n",
    "            false_positive_drug+=1\n",
    "                 \n",
    "    for i in range(NUM_DRUG):\n",
    "        if drug_matched[i] == 0:\n",
    "            false_negative_drug+=1\n",
    "        \n",
    "    row['Drug_TP']=fully_matched_drug\n",
    "    row['Drug_FP']=false_positive_drug\n",
    "    row['Drug_FN']=false_negative_drug\n",
    "    row['Drug_Paritial_Matched']=partial_overlap_drug\n",
    "    return row\n",
    "  \n",
    "\n",
    "\n",
    "\"\"\"data_ent_out =pd.read_csv('entity_extractor_model_'+str(extractor_to_use)+'_output_test4.csv')\n",
    "\n",
    "data_ent_out.dropna(how='all',inplace=True)\n",
    "#data_ent_out.dropna(how='all', axis=1, inplace=True)\n",
    "data_doc_out=pd.read_excel('Validation Dataset_filled.xlsx',engine='openpyxl')\n",
    "\n",
    "data_doc_out.dropna(how='all',inplace=True)\n",
    "#data_doc_out.dropna(how='all', axis=1, inplace=True)\n",
    "df_results = data_ent_out.merge(data_doc_out,on='Prescription id')\n",
    "\n",
    "df_results.fillna(\"\",inplace=True)\n",
    "df_results=df_results.apply(lambda row:generate_result_complaint(row),axis=1)\n",
    "df_results=df_results.apply(lambda row:generate_result_diagnosis(row),axis=1)\n",
    "df_results=df_results.apply(lambda row:generate_result_investigation(row),axis=1)\n",
    "df_results=df_results.apply(lambda row:generate_result_procedure(row),axis=1)\n",
    "df_results=df_results.apply(lambda row:generate_result_drugs(row),axis=1)\n",
    "\n",
    "\n",
    "df_results.to_csv('entity_extractor_model_'+str(extractor_to_use)+'_results_output_test4.csv')\"\"\"\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "#whole_data =pd.read_csv('Max_diagnosis_final.csv')\n",
    "#whole_data.dropna(how='all',inplace=True)\n",
    "#whole_data.dropna(how='all', axis=1, inplace=True)\n",
    "#whole_data.rename(columns = {'Unnamed: 0':'Prescription id'}, inplace = True)\n",
    "#data_ent_out =pd.read_csv('entity_extractor_model_'+str(extractor_to_use)+'_output_test.csv')\n",
    "#data_ent_out.dropna(how='all',inplace=True)\n",
    "#data_ent_out.dropna(how='all', axis=1, inplace=True)\n",
    "#data_ent_out=data_ent_out.merge(whole_data, on='Prescription id')\n",
    "#data_ent_out.to_csv('examples_to_send.csv')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#whole_data =pd.read_csv('bookiniddate.csv')\n",
    "#whole_data.dropna(how='all',inplace=True)\n",
    "#whole_data.dropna(how='all', axis=1, inplace=True)\n",
    "#examples =pd.read_csv('examples_to_send.csv')\n",
    "#examples.dropna(how='all',inplace=True)\n",
    "#examples.dropna(how='all', axis=1, inplace=True)\n",
    "#examples=examples.merge(whole_data, on='bookingid')\n",
    "#examples.to_csv('examples_to_send1.csv')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b126605a8bf1c668c00856828119e0e9f2c7c5cd6ed5cd17fd467699ab9b9d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
